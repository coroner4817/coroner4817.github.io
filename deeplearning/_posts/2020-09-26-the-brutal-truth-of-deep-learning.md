---
layout: post
title: The brutal truth of Deep Learning
date: 2020-09-26 04:02 -0400
categories: [deeplearning]
description: >
  Is Deep Learning the right path to ultimate AI? 
image:
  path: "/assets/img/blog/the-dark-side-of-the-moon.jpg"
---

Changing from 0.1f to 1.f is always easier than changing from 0.f to 0.1f. So for the first blog, I decided to translate a small article I wrote before into English and add some new thoughts. It is about my personal understanding of Deep Learning and its ethical influences on human civilization. Hope this can give you some heuristic ideas.  

* toc
{:toc .large-only}

### Deep Learning is not the true AI

Just giving you a heads up of the tone for this article: **I am pessimistic about this Deep Learning technology and I think it is diverging human beings from the path of inventing the true Artificial Intelligence.** I am not totally opposed to DL but just think it is a superficial solution for the true AI problem. I was once passionate about this new ML technology and have tried to apply it to all the medical image processing tasks I was doing in the research lab. Needless to say, the results were promising and amazing. It can automatically seize the hidden patterns of the dataset without understanding what the data really is. If we want to improve the result, just adding more layers and more neurons. You can see the performance is positively correlated. However, when I stepped out of the lab and started gazing at the real intelligence, like the kids learning language and playing games. I am starting wondering is this technology giving machines the ability to perceive the real world, or is it just a brute-force method to torture the computer, like the famous scene in the A Clockwork Orange.  

I am going with the latter. The argument is simple: **If you show kids less than 10 images of the cat, they can identify a cat on the street the next day.** They can even sketch a cat-like figure on paper. So the real intelligence entities don’t learn from a huge amount of data, but a small chunk of information is enough to stimulate the brain to memorize the pattern. They may still make mistakes. Kids might say a tiger is a big cat. But this level of mistakes is tolerable. We can very easily train the kid into a robust cat predict system. While comparing with Deep Learning, we have to throw tons of images, 20% positive, 80% negative, then we might get a model that still thinks a cookie is a chihuahua. So this process is unnatural and the true intelligence we are trying to simulate is not generated from this kind of brute-force way. Also as long as we are training using data set, the model is always overfitting, just to different extents. I think the world's most robust car predicting model can still fail on the image of the alien truck by Tesla. The real AI that we are all pursuing of, shouldn't working this way.

### How Deep Learning really works

My understanding of the traditional structured Deep Learning method is **a brute-force way to approximate the real model** for the given task. And the training process is a **non-linear cost function optimization**, solved using gradient descent, which is a typical solution for other modern computer vision problems like SLAM. DL uses a massive amount of parameters and complex data transform to fit the curve. It is cheap in the meaning that we don’t even need to analyze the type of the inner model. A lot of techniques pooling, ReLU or Dropout, just **break the linearity** feature of the model and **add more randomness** to itself, so it has a better chance to get a close result. This is no fundamental difference than randomly tossing a coin to decide if the image is a cat. Even the forward path of DL is similar to tossing a coin. From the output data size of each layer perspective, we can see that DL project the original data to higher dimensions through several transforms and then project the data back to the categorical dimension. Then we have our predict score. And when tossing a coin, we throw the coin to 3D space and when it falls on the ground, it is projected to the true or false dimensions. The spinning, speed, and external wind add randomness to this tossing procedure. So we are bet on luck all the time.

Then what is a natural way for a machine to learn? Let’s say our goal is to create an AI close to a kid’s intelligence. This problem actually is also a statistics problem. Instead of directly trying to understand the data, we can try to understand how kids are learning the data instead. We can observe the process of how kids are learning. We can obtain the output of the kids’ learning process, like what kids sketch on the paper when they are asked to draw a cat or ask kids to describe what is a cat. From these outputs, we might have a guess of the hidden sequential model within the brain. However, seems we are back in data mining again. I don’t think I can have any deeper insight into the solution. But I think this is a better approach to simulate true intelligence. 

### Accommodation to machine 

You can disagree with what I said above. But let’s still step back and think about a more practical problem: how much longer will we have a generalized reliable AI in our daily life? AI was portrait as the next technology revolution. Look at the innovations that really change human civilization. Inventors started designing cars around the 17th century, and it looks almost 300 years to make cars become an affordable common item in society. AI is much more complex than cars and we are still early stage and haven’t even defined what AI is. So we can set a bold assumption here, **maybe the AI doesn’t exist, or there will be a smart scientist proves that we need to faster than light to create AI.** So I think we really think about what if true AI is just a fantasy and humans can never create it.

Throughout years of the skeptic to DL, I start generating some ideas of **AI Accelerationism**. Accelerationism means that in order to achieve the goal, we can do whatever we want, even those consequences are negative to the other areas. This is a dangerous and selfish thought. But really productive and can improve humans' life. The idea is **if it is hard to make machines understand humans, then why not make humans accommodate machines. If error-less NLP is hard to achieve, then we can invent a very logical and formatted language with a well-defined grammar that machine can easily understand. Then we just make every kid learning this language and eventually humans can freely talk with the machine.** Similar ideas also apply to other aspects like microchipping all the human and product so that machines can directly recognize the object. Although this sounds crazy and impractical. But think about cars again. Before cars were introduced to society, people can walk anywhere they want. There was no traffic light or even the concept of the pedestrian. However, once cars started to appear on the street, people have to learn the rules. People have to accommodate to the machine. The government created laws to redefine humans' behavior on the streets and this expedited the revolution of the automotive industry. So why don’t we also create new rules to accommodate the AI?

Think from the other side, will you ever allow AI makes a decision for you? I don’t think people will trust the AI ever. The pride and prejudice of human being that baked inside humanity always remind us that machine is a tool, it will not tell us what to do. **Since AI is always more foolish than human and cannot solve problems that human cannot solve and nobody rely on AI to make decisions, then it is not significantly meaningful.** We have given too many expectations to it. If so why don’t we accelerate the creation of AI? **Accommodating to the machine is the fastest and simplest way that we can create workable AI in daily life. Then with this powerful tool, the human being can more focus on the problem that meaningful,** like the ultimate understanding of the universe or how to live immortally. AI cannot give the answer to those problems at all. Only the human being is the key to all the locks in the world. 

### Reveal the brutal truth of dark AI

So this Accelerationism definitely will have a downside to our life. **The created fake AI will make the human being loses humanity and culture.** The fascinating part of the world is that we are all different. All of the cultures are diverse. This is the best part of us but worst for the machine. **The high uncertainty and information entropy make the machine cannot have a reliable prior probability estimated.** So is the AI problem really a technology problem? It won’t be. AI will become a controversial paradox of the new order and old tradition. **A pandora's box that can open a door to the new cyberpunk world and sacrifice the beautiful humanity.**

Once you start viewing AI like this, you will understand the statement made by **Elon Musk that: “AI will be the biggest threat to the human being”.** It was sound funny before you read the article, right? The threat will not come from AI, but from our eagerness of evolving the human civilization and expanding our territory. 

### What should Deep Learning do and what should not

Finally, I want to express what we should deal with this Deep Learning technology. **I have to admit that it is a great data filter. We can definitely use it to do some non-critical tasks like super-resolution, photo-synthesis, language translation, etc.** The DLSS provided by Nvidia is a good example. I am imagining it is not trained with too much data. This type of task is very efficient when using DL. Other generative learning processes like GAN and Autoencoder also cannot really hurt us, but add more fun in the entertainment aspects. However, **AI shouldn't be involved in tasks like classification, medical, stock market, or self-driving. These areas require a strong decision-maker. AI just doesn’t have the ability to do so.**

If you find yourself reading this article in the 25th century and human being still speaks English, please leave a comment below to indicate if I was wrong or correct. 